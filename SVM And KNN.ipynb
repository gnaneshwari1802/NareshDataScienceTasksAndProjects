{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YYAcxoqyRpm"
      },
      "outputs": [],
      "source": [
        "# SVR\n",
        "#SVM regression or Support Vector Regression (SVR) is a machine learning algorithm used for regression analysis. It is different from traditional linear regression methods as it finds a hyperplane that best fits the data points in a continuous space, instead of fitting a line to the data points.\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv(r'C:\\Users\\kdata\\Desktop\\KODI WORK\\1. NARESH\\1. MORNING BATCH\\N_Batch -- 8.30AM\\3. MAY\\3rd\\EMP SAL.csv')\n",
        "X = dataset.iloc[:, 1:2].values\n",
        "y = dataset.iloc[:, 2].values\n",
        "\n",
        "\n",
        "# Fitting SVR to the dataset\n",
        "from sklearn.svm import SVR\n",
        "\"\"\"\n",
        "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. The advantages of support vector machines are: Effective in high dimensional spaces. Still effective in cases where number of dimensions is greater than the number of samples.\n",
        "What is SVR regression?\n",
        "Support Vector Regression In Machine Learning\n",
        "SVM regression or Support Vector Regression (SVR) is a machine learning algorithm used for regression analysis. It is different from traditional linear regression methods as it finds a hyperplane that best fits the data points in a continuous space, instead of fitting a line to the data\n",
        "\"\"\"\n",
        "regressor = SVR(kernel='poly', degree=7, gamma='auto')\n",
        "regressor.fit(X, y)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\"\"\"sklearn. neighbors provides functionality for unsupervised and supervised neighbors-based learning methods. Unsupervised nearest neighbors is the foundation of many other learning methods, notably manifold learning and spectral clustering.\n",
        "This algorithm is used to solve the classification model problems. K-nearest neighbor or K-NN algorithm basically creates an imaginary boundary to classify the data. When new data points come in, the algorithm will try to predict that to the nearest of the boundary line.\n",
        "\n",
        "What is the KNN regression?\n",
        "2 K-nearest Neighbours Regression | Machine Learning for ...\n",
        "KNN regression is a non-parametric method that, in an intuitive manner, approximates the association between independent variables and the continuous outcome by averaging the observations in the same neighbourhood.\n",
        "\"\"\"\n",
        "regressor_knn = KNeighborsRegressor(n_neighbors=6)\n",
        "regressor_knn.fit(X,y)\n",
        "\n",
        "#regressor created properly with default parameter after execute the above line of code\n",
        "#now we will check what was the actual salary after scaling\n",
        "\n",
        "# Predicting a new result\n",
        "\n",
        "y_pred_knn = regressor_knn.predict([[6.5]])\n",
        "\n",
        "\n",
        "y_pred_svr = regressor.predict([[6.5]])\n",
        "\n",
        "#y_pred = sc_y.inverse_transform(regressor.predict(sc_X.transform(np.array([[6.5]]))))\n",
        "#we will see what is predicted salary for the 6.5yrs of exp new employee\n",
        "#always check the next argument function by select the object inspector\n",
        "#you have to transform the 6.5 numerical value transform and fit to the regressor\n",
        "#we have to do the inverse transform to get the orginial scale & by using the inverse_transform then we will get the scaled prediction salary\n",
        "#after execute we get very great prediction we found the predicted sal is 170k which is too good\n",
        "#we can say that our svr model quite good model compare to polynomial regression, finally we can say that svr is great model\n",
        "\n",
        "\n",
        "# Visualising the SVR results\n",
        "plt.scatter(X, y, color = 'red')\n",
        "plt.plot(X, regressor.predict(X), color = 'blue')\n",
        "plt.title('Truth or Bluff (SVR)')\n",
        "plt.xlabel('Position level')\n",
        "plt.ylabel('Salary')\n",
        "plt.show()\n",
        "#if you check the output that is svr model & its predicting the each of the real observation\n",
        "#red points are real observation point & blue lines are predicted line & now you can say svr is fitted much better curve on the dataset\n",
        "#same hear if you check the ceo actual observation point but you will find as still we can improve the graph and lets see how can we do that in svr\n",
        "#in this case ceo is outlier hear becuase ceo is quite far from our observation, thats ok\n",
        "\n",
        "#what exactly we are doing hear to check the what exactly employees have 6.5yrs experience predict salary\n",
        "\n",
        "\n",
        "# Visualising the SVR results (for higher resolution and smoother curve)\n",
        "X_grid = np.arange(min(X), max(X), 0.01) # choice of 0.01 instead of 0.1 step because the data is feature scaled\n",
        "X_grid = X_grid.reshape((len(X_grid), 1))\n",
        "plt.scatter(X, y, color = 'red')\n",
        "plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')\n",
        "plt.title('Truth or Bluff (SVR)')\n",
        "plt.xlabel('Position level')\n",
        "plt.ylabel('Salary')\n",
        "plt.show()\n",
        "\n",
        "#great curve you got isn't it , same dataset you worked polynomial regressor & svm regressor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "3IUvhipDyVu_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(r'/content/EMP SAL.csv')\n",
        "X = dataset.iloc[:, 1:2].values\n",
        "y = dataset.iloc[:, 2].values"
      ],
      "metadata": {
        "id": "U2ibQ1PWyeSp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gla_e2QzykNU",
        "outputId": "44109719-716c-464d-8a79-324b49e60204"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1],\n",
              "       [ 2],\n",
              "       [ 3],\n",
              "       [ 4],\n",
              "       [ 5],\n",
              "       [ 6],\n",
              "       [ 7],\n",
              "       [ 8],\n",
              "       [ 9],\n",
              "       [10]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0qXWp7Jym0r",
        "outputId": "885d9dcd-c5e6-4323-d714-20fefadf289f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  45000,   50000,   60000,   80000,  110000,  150000,  200000,\n",
              "        300000,  500000, 1000000])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "\"\"\"\n",
        "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. The advantages of support vector machines are: Effective in high dimensional spaces. Still effective in cases where number of dimensions is greater than the number of samples.\n",
        "What is SVR regression?\n",
        "Support Vector Regression In Machine Learning\n",
        "SVM regression or Support Vector Regression (SVR) is a machine learning algorithm used for regression analysis. It is different from traditional linear regression methods as it finds a hyperplane that best fits the data points in a continuous space, instead of fitting a line to the data\n",
        "\"\"\"\n",
        "regressor = SVR(kernel='poly', degree=7, gamma='auto')\n",
        "regressor.fit(X, y)\n"
      ],
      "metadata": {
        "id": "KixMNlUIyrbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\"\"\"sklearn. neighbors provides functionality for unsupervised and supervised neighbors-based learning methods. Unsupervised nearest neighbors is the foundation of many other learning methods, notably manifold learning and spectral clustering.\n",
        "This algorithm is used to solve the classification model problems. K-nearest neighbor or K-NN algorithm basically creates an imaginary boundary to classify the data. When new data points come in, the algorithm will try to predict that to the nearest of the boundary line.\n",
        "\n",
        "What is the KNN regression?\n",
        "2 K-nearest Neighbours Regression | Machine Learning for ...\n",
        "KNN regression is a non-parametric method that, in an intuitive manner, approximates the association between independent variables and the continuous outcome by averaging the observations in the same neighbourhood.\n",
        "\"\"\"\n",
        "regressor_knn = KNeighborsRegressor(n_neighbors=6)\n",
        "regressor_knn.fit(X,y)\n"
      ],
      "metadata": {
        "id": "KBBtwmJGy89Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_knn = regressor_knn.predict([[6.5]])\n",
        "\n",
        "\n",
        "y_pred_svr = regressor.predict([[6.5]])\n"
      ],
      "metadata": {
        "id": "lc8BsbPTzCOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X, y, color = 'red')\n",
        "plt.plot(X, regressor.predict(X), color = 'blue')\n",
        "plt.title('Truth or Bluff (SVR)')\n",
        "plt.xlabel('Position level')\n",
        "plt.ylabel('Salary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5b2240aRzGyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_grid = np.arange(min(X), max(X), 0.01) # choice of 0.01 instead of 0.1 step because the data is feature scaled\n",
        "X_grid = X_grid.reshape((len(X_grid), 1))\n",
        "plt.scatter(X, y, color = 'red')\n",
        "plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')\n",
        "plt.title('Truth or Bluff (SVR)')\n",
        "plt.xlabel('Position level')\n",
        "plt.ylabel('Salary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EgIxbwT4zKbO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
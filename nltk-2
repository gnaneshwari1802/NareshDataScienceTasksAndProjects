import os
import nltk
# !pip install wordcloud
# !pip install nltk wordcloud
# !conda install -c conda-forge wordcloud
"""A corpus is a collection of papers written in the same language. It will be a collection of text files stored in a directory, frequently surrounded by other text file directories. In the nltk. data.
"""
"""
A corpus is a collection of papers written in the same language. It will be a collection of text files stored in a directory, frequently surrounded by other text file directories. In the nltk. data.
"""
#nltk.download() 
#import nltk.corpus
# we will see what is mean by corpora and what all are availabel in nltk python library
#print(os.listdir(nltk.data.find('corpora')))

#you get a lot of file , some of have some textual document, different function associated with that function , stopwords, differenent type of function 
#for our example i will lets take consideration as brown & we will understand what exactly nlp can do 
What is corpus in NLTK?
"""A corpus can be defined as a collection of text documents. It can be thought as just a bunch of text files in a directory, often alongside many other directories of text files. How it is done ? NLTK already defines a list of data paths or directories in nltk.
"""
#from nltk.corpus import brown
#brown.words() 
#nltk.corpus.brown.fileids()
#nltk.corpus.gutenberg
#nltk.corpus.gutenberg.fileids() 
# you can also create your own words 

AI = '''Artificial Intelligence refers to the intelligence of machines. This is in contrast to the natural intelligence of 
humans and animals. With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and 
problem-solving. Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines. 
It is probably the fastest-growing development in the World of technology and innovation. Furthermore, many experts believe
AI could solve major challenges and crisis situations.'''
AI
'Artificial Intelligence refers to the intelligence of machines. This is in contrast to the natural intelligence of \nhumans and animals. With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and \nproblem-solving. Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines. \nIt is probably the fastest-growing development in the World of technology and innovation. Furthermore, many experts believe\nAI could solve major challenges and crisis situations.'
type(AI) 
# str
from nltk.tokenize import word_tokenize 
AI_tokens = word_tokenize(AI)
AI_tokens

 

len(AI_tokens)
# 81
from nltk.tokenize import sent_tokenize 
AI_sent = sent_tokenize(AI)
AI_sent
# ['Artificial Intelligence refers to the intelligence of machines.',
#  'This is in contrast to the natural intelligence of \nhumans and animals.',
#  'With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and \nproblem-solving.',
#  'Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines.',
#  'It is probably the fastest-growing development in the World of technology and innovation.',
#  'Furthermore, many experts believe\nAI could solve major challenges and crisis situations.']
len(AI_sent)
# 6
AI
# 'Artificial Intelligence refers to the intelligence of machines. This is in contrast to the natural intelligence of \nhumans and animals. With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and \nproblem-solving. Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines. \nIt is probably the fastest-growing development in the World of technology and innovation. Furthermore, many experts believe\nAI could solve major challenges and crisis situations.'
from nltk.tokenize import blankline_tokenize # GiVE YOU HOW MANY PARAGRAPH
AI_blank = blankline_tokenize(AI) 
AI_blank
#AI_blank
# ['Artificial Intelligence refers to the intelligence of machines. This is in contrast to the natural intelligence of \nhumans and animals. With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and \nproblem-solving. Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines. \nIt is probably the fastest-growing development in the World of technology and innovation. Furthermore, many experts believe\nAI could solve major challenges and crisis situations.']
len(AI_blank) 
1
# NEXT WE WILL SEE HOW WE WILL USE UNI-GRAM,BI-GRAM,TRI-GRAM USING NLTK

from nltk.util import bigrams,trigrams,ngrams 
string = 'the best and most beautifull thing in the world cannot be seen or even touched,they must be felt with heart'
quotes_tokens = nltk.word_tokenize(string)
quotes_tokens

len(quotes_tokens)
# 23
quotes_bigrams = list(nltk.bigrams(quotes_tokens))
quotes_bigrams
# [('the', 'best'),
#  ('best', 'and'),
#  ('and', 'most'),
#  ('most', 'beautifull'),
#  ('beautifull', 'thing'),
#  ('thing', 'in'),
#  ('in', 'the'),
#  ('the', 'world'),
#  ('world', 'can'),
#  ('can', 'not'),
#  ('not', 'be'),
#  ('be', 'seen'),
#  ('seen', 'or'),
#  ('or', 'even'),
#  ('even', 'touched'),
#  ('touched', ','),
#  (',', 'they'),
#  ('they', 'must'),
#  ('must', 'be'),
#  ('be', 'felt'),
#  ('felt', 'with'),
#  ('with', 'heart')]
quotes_tokens
# ['the',
#  'best',
#  'and',
#  'most',
#  'beautifull',
#  'thing',
#  'in',
#  'the',
#  'world',
#  'can',
#  'not',
#  'be',
#  'seen',
#  'or',
#  'even',
#  'touched',
#  ',',
#  'they',
#  'must',
#  'be',
#  'felt',
#  'with',
#  'heart']
quotes_trigrams = list(nltk.trigrams(quotes_tokens))
quotes_trigrams
# [('the', 'best', 'and'),
#  ('best', 'and', 'most'),
#  ('and', 'most', 'beautifull'),
#  ('most', 'beautifull', 'thing'),
#  ('beautifull', 'thing', 'in'),
#  ('thing', 'in', 'the'),
#  ('in', 'the', 'world'),
#  ('the', 'world', 'can'),
#  ('world', 'can', 'not'),
#  ('can', 'not', 'be'),
#  ('not', 'be', 'seen'),
#  ('be', 'seen', 'or'),
#  ('seen', 'or', 'even'),
#  ('or', 'even', 'touched'),
#  ('even', 'touched', ','),
#  ('touched', ',', 'they'),
#  (',', 'they', 'must'),
#  ('they', 'must', 'be'),
#  ('must', 'be', 'felt'),
#  ('be', 'felt', 'with'),
#  ('felt', 'with', 'heart')]
# quotes_trigrams = list(nltk.ngrams(quotes_tokens))
# quotes_trigrams
# ---------------------------------------------------------------------------

quotes_ngrams = list(nltk.ngrams(quotes_tokens, 4)) 
quotes_ngrams

#it has given n-gram of length 4
len(quotes_tokens)
23
quotes_ngrams_1 = list(nltk.ngrams(quotes_tokens, 5)) 
quotes_ngrams_1

quotes_ngrams = list(nltk.ngrams(quotes_tokens, 9)) 
quotes_ngrams

# Next we need to make some changes in tokens and that is called as stemming, stemming will gives you root form of an word
# also we will see some root form of the word & limitation of the word

#porter-stemmer
from nltk.stem import PorterStemmer
pst = PorterStemmer()
pst.stem('having') #stem will gives you the root form of the word 
# 'have'
pst.stem('affection')
# 'affect'
pst.stem('playing')
# 'play'
pst.stem('give') 
# 'give'
words_to_stem=['give','giving','given','gave']
for words in words_to_stem:
    print(words+  ':' + pst.stem(words))

pst.stem('playing')
# 'play'
words_to_stem=['give','giving','given','gave','thinking', 'loving', 'final', 'finalized', 'finally']
# i am giving these different words to stem, using porter stemmer we get the output

for words in words_to_stem:
    print(words+ ':' +pst.stem(words))
    
#in porterstemmer removes ing and replaces with e
# give:give
# giving:give
# given:given
# gave:gave
# thinking:think
# loving:love
# final:final
# finalized:final
# finally:final
#another stemmer known as lencastemmer stemmer and lets see what the different we will get hear
#stem the same thing using lencastemmer

from nltk.stem import LancasterStemmer
"""
What is LancasterStemmer?
Lancaster Stemming Algorithm

Like the Porter stemmer, the Lancaster stemmer consists of a set of rules where each rule specifies either deletion or replacement of an ending. Also, some rules are restricted to intact words, and some rules are applied iteratively as the word goes through them.
"""
"""
What is the use of stemming?
Stemming & Lemmatization
Stemming is a technique used to extract the base form of the words by removing affixes from them. It is just like cutting down the branches of a tree to its stems. For example, the stem of the words eating, eats, eaten is eat. Search engines use stemming for indexing the words.
"""
"""
What is stemming and lemmatization?
Stemming is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.
"""
"""
Porter algorithm is the least aggressive algorithm which is why is widely used and is the oldest stemming algorithm. The stems of the words are somewhat intuitive and are understandable. On the other hand, Lancaster algorithm is very aggressive because of its strictly chopping words and making it much confusing. With this algorithm in use, the stems become non-relatable to some extent and for this very reason, it is least used.
This is how Porter Stemmer works
image

This is how Lancaster Stemmer works.
"""

"""
In this tutorial, we’ll explain one methodology in natural language processing (NLP) that we can use as a text preprocessing step in our projects. Specifically, it’s about stemming and the difference between Porter and Lancaster stemming algorithms.

In most tutorials, stemming is presented as a process of reducing words into base form but rarely or never are mentioned different types of stemming methods.
"""
"""
Natural Languages Processing (NLP)
Natural language processing (NLP) is a field of computer science and artificial intelligence that deals with the relationships between computers and human (natural) languages. In particular, the goal of NLP is how to program computers to process and analyze large amounts of natural language data in the same way that a human does. It has grown in importance in recent years due to the availability of digital text data and new statistical techniques for language processing.

There are many examples of NLP projects, but some of the most popular are related to:

Sentiment analysis
Speech recognition
Machine translation
Text summarization
Chatbots
Named entity recognition
Search engines and others
As a result, these are very diverse tasks that require different text preprocessing techniques.

"""
"""
How to Preprocess Text in NLP?
For a computer to understand text, we would need to convert it into numbers and then apply some mathematical operations. But before that, there are some preprocessing steps that are common to many NLP projects:

Text cleaning – excluding from the text all symbols different than alphabet letters, numbers and punctuation marks, removing double white spaces, converting text into lowercase and similar.
Stop words removal – filter out some frequent words that don’t give any useful information.
Lemmatization – grouping together different forms of the same word into its base form. For instance, converting the words “studying”, “studies” and “studied” into “study”.
Stemming – the process of reducing a word to its base or root form, also known as word stem.
"""
"""
What Is Stemming?
In brief, stemming is the process of reducing a word to its word stem. Word stem is a base or root form of the word and doesn’t need to be an existing word. For example, the Porter algorithm reduces the words “argue”, “argued”, “argues” and “arguing” to the stem “argu” which isn’t an existing word.

With stemming, we’re able to extract meaningful information from vast sources, like big data, and afterward help search engines to query information. It’s possible to get more results if we recognize and search more word forms. Also, when a word form is recognized, it may be possible to return search results that would otherwise be missed. Because of that, stemming is essential to search queries since, with stemming, we’re able to retrieve additional information.
"""
"""
 How Does Stemming Work?
There are many ways how stemming algorithms work. Some simple methods will only recognize prefixes and suffixes and strip them. Because of this simplicity, they are prone to errors. In many cases, they will strip wrong prefixes and suffixes. Also, it might be difficult to handle some word forms like irregular verbs, for example, words such as “saw” and “see”.

More complex stemming algorithms use lookup tables of different word forms in combination with well-known suffixes and prefixes. Some of them firstly determine the part of speech of a word and after apply different normalization rules for each part of speech.
"""
"""
Porter Stemming Algorithm
Porter stemmer is a suffix stripping algorithm. In short, it uses predefined rules to strip words into their base forms.

Every word can be represented as a sequence of consonants and vowels. Let’s denote a consonant with “c”, and a sequence of consonants of length greater than 0 with “C”. Similarly, “v” is a vowel and “V” a sequence of vowels of length greater than 0. Then, every word has one of the four forms

CVCV…C
CVCV…V
VCVC…C
VCVC…V
or as a single form

[C]VCVC...[V]


freestar
where square brackets denote the arbitrary presence of their contents. The above expression also can be written as

[C][VC]^{m}[V]

where m is called the measure of the word. Some world examples with different m are:

m = 0 (tree, by, why)
m = 1 (oats, trees, ivy)
m = 2 (private, oaten, banana)
To remove common suffixes, Porter stemmer applies more than 50 rules, grouped in 5 steps and some substeps. All rules have a form

(condition) S1 -> S2

This means that if a word has the suffix S1 and the part before suffix (stem) satisfies the condition, we replace S1 with S2. Also, some rules don’t have conditions. Below are some rules with word stemming examples:

SSES -> SS (caresses -> caress)
S ->  (cats -> cat)
(m > 0) EED -> EE (agreed -> agree, feed -> feed)
(m > 0) ATOR -> ATE (operator -> operate)
(m > 1) ER ->  (airliner -> airlin)
(m > 1 and (*S or *T)) ION ->  (adoption -> adopt)

"""
"""
Lancaster Stemming Algorithm
Lancaster is one of the most aggressive stemmers as it tends to over stem many words. Like the Porter stemmer, the Lancaster stemmer consists of a set of rules where each rule specifies either deletion or replacement of an ending. Also, some rules are restricted to intact words, and some rules are applied iteratively as the word goes through them.

Because of more strict rules, there are two additional conditions to prevent the stemming of various short-rooted words:


freestar
If the word starts with a vowel, then at least two letters must remain after stemming (owing -> ow, but not ear -> e).
If the word starts with a consonant, then at least three letters must remain after stemming, and at least one of these must be a vowel or “y” (saying -> say, but not string -> str).
Lancaster stemmer has more than 100 rules, around double that of Porter stemmer. Also, the authors defined rules using different notation than Porter’s stemming rules. Each rule has five components, two of which are optional:

[ending in reverse][optional intact flag “*”][remove total letters][optional append string][continuation symbol, “>” or “.”]

In particular, here are some examples of the rules:

“sei3y>” – if the word ends with “ies”, then replace the last three letters with “y” and then apply the stemmer again to truncated form.
“mu*2.” – if the word ends with “um” and if the word is intact, then remove the last 2 letters and terminate.
“nois4j>” – replace the ending “sion” with “j” and apply the stemmer again.

"""
"""
Porter is the most popular stemming algorithm, and it’s a default option in most NLP projects.
Lancaster is one of the most aggressive stemming methods. Approximately, it has two times more stemming rules than the Porter method and tends to over stem a lot of words.
In most NLP projects, Porter’s stemming algorithm will give more meaningful results, but sometimes Lancaster stemmer might be worth trying.
"""
lst = LancasterStemmer()
for words in words_to_stem:
    print(words + ':' + lst.stem(words))
    
# lancasterstemmer is more aggresive then the porterstemmer
# give:giv
# giving:giv
# given:giv
# gave:gav
# thinking:think
# loving:lov
# final:fin
# finalized:fin
# finally:fin
words_to_stem=['give','giving','given','gave','thinking', 'loving', 'final', 'finalized', 'finally']
# i am giving these different words to stem, using porter stemmer we get the output

for words in words_to_stem:
    print(words+ ':' +pst.stem(words))
# give:give
# giving:give
# given:given
# gave:gave
# thinking:think
# loving:love
# final:final
# finalized:final
# finally:final
#we have another stemmer called as snowball stemmer lets see about this snowball stemmer

from nltk.stem import SnowballStemmer
sbst = SnowballStemmer('english')
for words in words_to_stem:
    print(words+ ':' +sbst.stem(words))
    
#snowball stemmer is same as portstemmer
#different type of stemmer used based on different type of task
#if you want to see how many type of giv has occured then we will see the lancaster stemmer
# give:give
# giving:give
# given:given
# gave:gave
# thinking:think
# loving:love
# final:final
# finalized:final
# finally:final
#sometime stemming does not work & lets say e.g - fish,fishes & fishing all of them belongs to root word fish, 
#one hand stemming will cut the end & lemmatization will take into the morphological analysis of the word

from nltk.stem import wordnet
from nltk.stem import WordNetLemmatizer
word_lem = WordNetLemmatizer()

#Hear we are going to wordnet dictionary & we are going to import the wordnet lematizer
words_to_stem
#word_lem.lemmatize('corpora') #we get output as corpus 

#refers to a collection of texts. Such collections may be formed of a single language of texts, or can span multiple languages -- there are numerous reasons for which multilingual corpora (the plural of corpus) may be useful

for words in words_to_stem:
    print(words+ ':' +word_lem.lemmatize(words))

pst.stem('final')

lst.stem('finally')

sbst.stem('finalized')

lst.stem('final')

lst.stem('finalized')

# there is other concept called POS (part of speech) which deals with subject, noun, pronoun but before of this lets go with other concept called STOPWORDS
# STOPWORDS = i, is, as,at, on, about & nltk has their own list of stopewords 

from nltk.corpus import stopwords
stopwords.words('english') 

len(stopwords.words('english')) 

stopwords.words('spanish')

len(stopwords.words('spanish')) 

stopwords.words('french') 



len(stopwords.words('french')) 

stopwords.words('german') 

len(stopwords.words('german'))

# stopwords.words('hindi') # research phase 
# stopwords.words('marathi') 

# stopwords.words('telugu') 

import re
punctuation = re.compile(r'[-.?!,:;()|0-9]') 

#now i am going to create to empty list and append the word without any punctuation & naming this as a post punctuation
punctuation=re.compile(r'[-.?!,:;()|0-9]', re.UNICODE)
AI
AI_tokens
len(AI_tokens)
# we will see how to work in POS using NLTK library

sent = 'kathy is a natural when it comes to drawing'
sent_tokens = word_tokenize(sent)
sent_tokens

# first we will tokenize usning word_tokenize & then we will use pos_tag on all of the tokens 

for token in sent_tokens:
    print(nltk.pos_tag([token]))

sent2 = 'john is eating a delicious cake'
sent2_tokens = word_tokenize(sent2)

for token in sent2_tokens:
    print(nltk.pos_tag([token]))

# Another concept of POS is called NER ( NAMED ENTITIY RECOGNITION ), NER is the process of detecting name such as movie, moneytary value,organiztion, location, quantities & person
# there are 3 phases of NER - ( 1ST PHASE IS - NOUN PHRASE EXTRACTION OR NOUN PHASE IDENTIFICATION - This step deals with extract all the noun phrases from text using dependencies parsing and pos tagging
# 2nd step we have phrase classification - this is the classification where all the extracted nouns & phrase are classified into category such as location,names and much more 
# some times entity are misclassification 
# so if you are use NER in python then you need to import NER_CHUNK from nltk library
from nltk import ne_chunk
NE_sent = 'The US president stays in the WHITEHOUSE '
NE_tokens = word_tokenize(NE_sent)

#after tokenize need to add the pos tags
NE_tokens

NE_tags = nltk.pos_tag(NE_tokens)
NE_tags

#we are passin the NE_NER into ne_chunks function and lets see the outputs

NE_NER = ne_chunk(NE_tags)
print(NE_NER)

new = 'the big cat ate the little mouse who was after fresh cheese'
new_tokens = nltk.pos_tag(word_tokenize(new))
new_tokens

# tokenize done and lets add the pos tags also
# Libraries
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# Create a list of word
text=("Python Python Python Matplotlib Matplotlib Seaborn Network Plot Violin Chart Pandas Datascience Wordcloud Spider Radar Parrallel Alpha Color Brewer Density Scatter Barplot Barplot Boxplot Violinplot Treemap Stacked Area Chart Chart Visualization Dataviz Donut Pie Time-Series Wordcloud Wordcloud Sankey Bubble")
text
# Create the wordcloud object
wordcloud = WordCloud(width=480, height=480, margin=0).generate(text) 
# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.margins(x=0, y=0)
plt.show()
